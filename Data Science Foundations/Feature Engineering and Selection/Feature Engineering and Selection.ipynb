{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb7c84c2",
   "metadata": {},
   "source": [
    "# Feature Engineering and Selection\n",
    "\n",
    "Feature engineering and selection are crucial steps in the machine learning pipeline to improve model performance. Let's go through an example using a hypothetical dataset. In this example, we'll create a synthetic dataset with some features and then demonstrate feature engineering and selection.\n",
    "\n",
    "**Feature Engineering**\n",
    "\n",
    "We create new features (```interaction_feature```, ```binned_feature```, and one-hot encoding for ```feature_4```) based on existing features.\n",
    "\n",
    "**Train-Test Split**\n",
    "\n",
    "We split the dataset into training and testing sets.\n",
    "\n",
    "**Train a Model**\n",
    "\n",
    "We train a Random Forest model on the original dataset.\n",
    "\n",
    "**Feature Selection**\n",
    "\n",
    "We use the feature importances provided by the model to select important features. In this case, we set a threshold of $0.05$ to select features.\n",
    "\n",
    "**Evaluate the Model with Selected Features**\n",
    "\n",
    "We train a new model using only the selected features and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8822d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   feature_1  feature_2  feature_3 feature_4  target\n",
      "0   0.374540   0.177701          0         B       0\n",
      "1   0.950714  -1.335344          0         A       0\n",
      "2   0.731994   0.380198          0         C       0\n",
      "3   0.598658   0.610586          0         C       1\n",
      "4   0.156019   0.559790          0         B       1\n",
      "\n",
      "Dataset after Feature Engineering:\n",
      "   feature_1  feature_2  feature_3  interaction_feature  bin_Q1  bin_Q2  \\\n",
      "0   0.374540   0.177701          0             0.066556       0       1   \n",
      "1   0.950714  -1.335344          0            -1.269531       0       0   \n",
      "2   0.731994   0.380198          0             0.278303       0       0   \n",
      "3   0.598658   0.610586          0             0.365532       0       0   \n",
      "4   0.156019   0.559790          0             0.087338       1       0   \n",
      "\n",
      "   bin_Q3  bin_Q4  one_hot_A  one_hot_B  one_hot_C  \n",
      "0       0       0          0          1          0  \n",
      "1       0       1          1          0          0  \n",
      "2       1       0          0          0          1  \n",
      "3       1       0          0          0          1  \n",
      "4       0       0          0          1          0  \n",
      "\n",
      "Feature Importances:\n",
      "                Feature  Importance\n",
      "3   interaction_feature    0.294301\n",
      "0             feature_1    0.293563\n",
      "1             feature_2    0.292440\n",
      "2             feature_3    0.033387\n",
      "9             one_hot_B    0.016958\n",
      "10            one_hot_C    0.016140\n",
      "8             one_hot_A    0.016004\n",
      "5                bin_Q2    0.010549\n",
      "6                bin_Q3    0.009893\n",
      "7                bin_Q4    0.009321\n",
      "4                bin_Q1    0.007444\n",
      "\n",
      "Selected Features:\n",
      "['interaction_feature', 'feature_1', 'feature_2']\n",
      "\n",
      "Model Accuracy with Selected Features: 0.49\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a synthetic dataset\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame({\n",
    "    'feature_1': np.random.rand(1000),\n",
    "    'feature_2': np.random.randn(1000),\n",
    "    'feature_3': np.random.randint(0, 2, size=1000),\n",
    "    'feature_4': np.random.choice(['A', 'B', 'C'], size=1000),\n",
    "    'target': np.random.randint(0, 2, size=1000)\n",
    "})\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Step 1: Feature Engineering\n",
    "# Let's create some new features based on existing ones\n",
    "\n",
    "# Example 1: Interaction feature\n",
    "X['interaction_feature'] = X['feature_1'] * X['feature_2']\n",
    "\n",
    "# Example 2: Binning numerical feature\n",
    "X['binned_feature'] = pd.cut(X['feature_1'], bins=[0, 0.25, 0.5, 0.75, 1], labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "# One-hot encoding the binned_feature\n",
    "X = pd.get_dummies(X, columns=['binned_feature'], prefix='bin')\n",
    "\n",
    "# Example 3: One-hot encoding categorical feature\n",
    "X = pd.get_dummies(X, columns=['feature_4'], prefix='one_hot')\n",
    "\n",
    "# Display the dataset after feature engineering\n",
    "print(\"\\nDataset after Feature Engineering:\")\n",
    "print(X.head())\n",
    "\n",
    "# Step 2: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train a model (Random Forest for example)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Feature Selection using feature importances\n",
    "# We can use the feature importances provided by the model to select important features\n",
    "\n",
    "# Display feature importances\n",
    "feature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(feature_importances)\n",
    "\n",
    "# Select features with importance greater than a threshold (e.g., 0.05)\n",
    "selected_features = feature_importances[feature_importances['Importance'] > 0.05]['Feature'].tolist()\n",
    "\n",
    "# Display selected features\n",
    "print(\"\\nSelected Features:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Step 5: Evaluate the model with selected features\n",
    "# Train the model with selected features\n",
    "model_selected_features = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_selected_features.fit(X_train[selected_features], y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model_selected_features.predict(X_test[selected_features])\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nModel Accuracy with Selected Features:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946ce6a",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Effective feature engineering and selection play pivotal roles in enhancing the performance of machine learning models. In the presented Python example, we started with a synthetic dataset and showcased various feature engineering techniques, such as creating interaction features, binning numerical variables, and one-hot encoding categorical features. Subsequently, a Random Forest model was trained on the original dataset, and feature importances were used to guide feature selection. By establishing a threshold for importance, we identified and retained the most relevant features, leading to a more focused and potentially more interpretable model. This process demonstrates the iterative and exploratory nature of feature engineering, where domain knowledge and experimentation contribute to refining the set of input features, ultimately improving model accuracy and generalization to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
